{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KrXo_Ng0wx2B"},"outputs":[],"source":["from datasets import load_dataset, DatasetDict, Audio\n","from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline\n","import torch\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","import evaluate\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Выбор переменных для обучения\n","datasets = \"mozilla-foundation/common_voice_11_0\"   # Из какого датасета берем данные для обучения\n","lang_datasets = \"ru\"                                # Выбираем язык датасета\n","wisp_model = \"openai/whisper-small\"                 # Выбираем базовую модель\n","wisp_lang = \"Russian\"                               # Выбираем язык базовой модели\n","output_dir = \"./whisper-small-ru\"                   # Выбираем путь сохранения результатов обучения"]},{"cell_type":"markdown","metadata":{"id":"BLGkNbmtxrTm"},"source":["# Подготовка данных и загрузка датасета"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAFuOqBiwx2M","outputId":"844fe769-2c1c-4b09-bfd6-9229bb202130"},"outputs":[{"name":"stderr","output_type":"stream","text":["f:\\prog\\anaconda\\envs\\fpmi_py\\lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n","        num_rows: 32491\n","    })\n","    test: Dataset({\n","        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n","        num_rows: 9630\n","    })\n","})\n"]}],"source":["common_voice = DatasetDict()\n","\n","common_voice[\"train\"] = load_dataset(datasets, lang_datasets, split=\"train+validation\")\n","common_voice[\"test\"] = load_dataset(datasets, lang_datasets, split=\"test\")\n","\n","print(common_voice)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYOlDWMmwx2O","outputId":"d5099620-e7ae-4f5d-9606-d7954e7dc84a"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 32491\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 9630\n","    })\n","})\n"]}],"source":["common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n","\n","print(common_voice)"]},{"cell_type":"markdown","metadata":{"id":"xXIKa6Dzx4Qy"},"source":["# Подготовка процессора, токенизатора и экстрактора признаков"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ub37U9Uwx2R"},"outputs":[],"source":["# Предварительно обрабатывавем входные данные\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ESii8AOwx2S"},"outputs":[],"source":["# Обрабатываем выходные данные модели в текстовый формат\n","tokenizer = WhisperTokenizer.from_pretrained(wisp_model, language=wisp_lang, task=\"transcribe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CkcEu_Vywx2S"},"outputs":[],"source":["# Наследует токенизатор и экстрактор - нужно для работы модели\n","processor = WhisperProcessor.from_pretrained(wisp_model, language=wisp_lang, task=\"transcribe\")"]},{"cell_type":"markdown","metadata":{"id":"3DhBZc2qyDr4"},"source":["Согласуем частоту дискретизации нашего аудио с частотой дискретизации модели(16 кГц)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVLXLoZDwx2U"},"outputs":[],"source":["common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYn9LJH5wx2W","outputId":"7295add9-9ffe-4cd8-edf4-8a1b42edc6fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'audio': {'path': 'C:\\\\Users\\\\Dronxix\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\bbacf3b2c63dcd87076bb896cdd00714ee4220b4078baf7a1f4d68a374bf7740\\\\ru_train_0/common_voice_ru_26426765.mp3', 'array': array([-5.68434189e-14, -1.81898940e-12, -1.70530257e-12, ...,\n","        9.95262781e-07, -1.48648405e-06, -2.20581842e-06]), 'sampling_rate': 16000}, 'sentence': 'Демократия неумолимо продвигается по Африке, и «арабская весна» была ее кульминацией.'}\n"]}],"source":["print(common_voice[\"train\"][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcXScWJ2wx2Z"},"outputs":[],"source":["def prepare_dataset(batch):\n","    from transformers import WhisperFeatureExtractor, WhisperTokenizer\n","    feature_extractor = WhisperFeatureExtractor.from_pretrained(wisp_model)\n","    tokenizer = WhisperTokenizer.from_pretrained(wisp_model, language=\"Russian\", task=\"transcribe\")\n","\n","    # Загружаем предобработанное аудио\n","    audio = batch[\"audio\"]\n","\n","    # Вычисляем входные признаки логарифмической спектрограммы Mel из аудиомассива.\n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","\n","    # Кодируем текст в идентификаторы меток\n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blM_jg0Cwx2c"},"outputs":[],"source":["common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=8)"]},{"cell_type":"markdown","metadata":{"id":"NqJYjHEp949-"},"source":["# Создаем сборщика данных"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlLJ_U4Lwx2d"},"outputs":[],"source":["@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # Разделяем входные признаки и лэйблы\n","        # Возвращаем pythorch тензоры для признаков\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # Получаем токенизированные лейблы\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # Увеличиваем через .pad лэйблы домаксимальной длинны\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # Заменяемм паддинги на -100, что бы не учитывать лейблы при вычислении потерь\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # Затем вырезаем токен начала транскрипта из начала последовательности лэйблов,\n","        # так как мы добавляем его позже во время обучения.\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-SFJkVpwx2e"},"outputs":[],"source":["# Инициализируем сборщика данных\n","data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"markdown","metadata":{"id":"Ccr6UD7zE7fN"},"source":["# Метрики оценки"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyBjOWdZwx2e"},"outputs":[],"source":["# Загрузка метрики WER\n","\n","metric = evaluate.load(\"wer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfeLZCJFwx2f"},"outputs":[],"source":["# Функция расчета метрик\n","def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # Заменяем -100 на pad_token_id в label_ids\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"]},{"cell_type":"markdown","metadata":{"id":"KAF7Iih-ErZB"},"source":["# Загрузка предобученной модели"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-c2XPXs8wx2f"},"outputs":[],"source":["model = WhisperForConditionalGeneration.from_pretrained(wisp_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0699J26wx2g"},"outputs":[],"source":["model.config.forced_decoder_ids = None\n","model.config.suppress_tokens = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIq1hQGpwx2g"},"outputs":[],"source":["# Задаем аргументы для обучения модели\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=output_dir,  # Название папки куда сохранить веса и чекпоинты\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXG5LXEOwx2g"},"outputs":[],"source":["# Тренер для обучения модели\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=common_voice[\"train\"],\n","    eval_dataset=common_voice[\"test\"],\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMFPxv4xwx2g"},"outputs":[],"source":["# Сохраняем предобученное состояние модели\n","processor.save_pretrained(training_args.output_dir)"]},{"cell_type":"markdown","metadata":{"id":"4PeJu_GsEjVB"},"source":["# Запуск обучения"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["ad132bd76ca34d73a0679661bece9628","da7952e4dd754a54828453827ff56a58","72bbb2b7b915485592a23d154cba2d2f"]},"id":"7m8O1dY8wx2h","outputId":"6bd88a38-97a3-483f-eb12-e36e200355ee"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad132bd76ca34d73a0679661bece9628","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["f:\\prog\\anaconda\\envs\\fpmi_py\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.929, 'learning_rate': 4.2000000000000006e-07, 'epoch': 0.01}\n","{'loss': 2.1936, 'learning_rate': 9.200000000000001e-07, 'epoch': 0.02}\n","{'loss': 1.4827, 'learning_rate': 1.42e-06, 'epoch': 0.04}\n","{'loss': 0.8146, 'learning_rate': 1.9200000000000003e-06, 'epoch': 0.05}\n","{'loss': 0.5925, 'learning_rate': 2.42e-06, 'epoch': 0.06}\n","{'loss': 0.47, 'learning_rate': 2.92e-06, 'epoch': 0.07}\n","{'loss': 0.4253, 'learning_rate': 3.4200000000000007e-06, 'epoch': 0.09}\n","{'loss': 0.3741, 'learning_rate': 3.920000000000001e-06, 'epoch': 0.1}\n","{'loss': 0.3053, 'learning_rate': 4.42e-06, 'epoch': 0.11}\n","{'loss': 0.2358, 'learning_rate': 4.92e-06, 'epoch': 0.12}\n","{'loss': 0.2279, 'learning_rate': 5.420000000000001e-06, 'epoch': 0.14}\n","{'loss': 0.2073, 'learning_rate': 5.92e-06, 'epoch': 0.15}\n","{'loss': 0.1829, 'learning_rate': 6.42e-06, 'epoch': 0.16}\n","{'loss': 0.2112, 'learning_rate': 6.92e-06, 'epoch': 0.17}\n","{'loss': 0.2195, 'learning_rate': 7.420000000000001e-06, 'epoch': 0.18}\n","{'loss': 0.1959, 'learning_rate': 7.92e-06, 'epoch': 0.2}\n","{'loss': 0.1821, 'learning_rate': 8.42e-06, 'epoch': 0.21}\n","{'loss': 0.1954, 'learning_rate': 8.920000000000001e-06, 'epoch': 0.22}\n","{'loss': 0.2017, 'learning_rate': 9.42e-06, 'epoch': 0.23}\n","{'loss': 0.1815, 'learning_rate': 9.920000000000002e-06, 'epoch': 0.25}\n","{'loss': 0.186, 'learning_rate': 9.940000000000001e-06, 'epoch': 0.26}\n","{'loss': 0.1768, 'learning_rate': 9.86857142857143e-06, 'epoch': 0.27}\n","{'loss': 0.1992, 'learning_rate': 9.797142857142858e-06, 'epoch': 0.28}\n","{'loss': 0.1752, 'learning_rate': 9.725714285714287e-06, 'epoch': 0.3}\n","{'loss': 0.188, 'learning_rate': 9.654285714285716e-06, 'epoch': 0.31}\n","{'loss': 0.1861, 'learning_rate': 9.582857142857143e-06, 'epoch': 0.32}\n","{'loss': 0.182, 'learning_rate': 9.511428571428572e-06, 'epoch': 0.33}\n","{'loss': 0.2038, 'learning_rate': 9.440000000000001e-06, 'epoch': 0.34}\n","{'loss': 0.1762, 'learning_rate': 9.368571428571428e-06, 'epoch': 0.36}\n","{'loss': 0.1769, 'learning_rate': 9.297142857142857e-06, 'epoch': 0.37}\n","{'loss': 0.1766, 'learning_rate': 9.225714285714286e-06, 'epoch': 0.38}\n","{'loss': 0.1892, 'learning_rate': 9.154285714285715e-06, 'epoch': 0.39}\n","{'loss': 0.1609, 'learning_rate': 9.082857142857143e-06, 'epoch': 0.41}\n","{'loss': 0.1776, 'learning_rate': 9.011428571428572e-06, 'epoch': 0.42}\n","{'loss': 0.1723, 'learning_rate': 8.94e-06, 'epoch': 0.43}\n","{'loss': 0.1733, 'learning_rate': 8.86857142857143e-06, 'epoch': 0.44}\n","{'loss': 0.1961, 'learning_rate': 8.797142857142857e-06, 'epoch': 0.46}\n","{'loss': 0.197, 'learning_rate': 8.725714285714286e-06, 'epoch': 0.47}\n","{'loss': 0.1836, 'learning_rate': 8.654285714285715e-06, 'epoch': 0.48}\n","{'loss': 0.1643, 'learning_rate': 8.582857142857144e-06, 'epoch': 0.49}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da7952e4dd754a54828453827ff56a58","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1204 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.21552051603794098, 'eval_wer': 43.83486372960828, 'eval_runtime': 5786.9935, 'eval_samples_per_second': 1.664, 'eval_steps_per_second': 0.208, 'epoch': 0.49}\n"]},{"name":"stderr","output_type":"stream","text":["f:\\prog\\anaconda\\envs\\fpmi_py\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.1722, 'learning_rate': 8.511428571428571e-06, 'epoch': 0.5}\n","{'loss': 0.1749, 'learning_rate': 8.44e-06, 'epoch': 0.52}\n","{'loss': 0.1768, 'learning_rate': 8.36857142857143e-06, 'epoch': 0.53}\n","{'loss': 0.1606, 'learning_rate': 8.297142857142859e-06, 'epoch': 0.54}\n","{'loss': 0.1773, 'learning_rate': 8.225714285714288e-06, 'epoch': 0.55}\n","{'loss': 0.1613, 'learning_rate': 8.154285714285715e-06, 'epoch': 0.57}\n","{'loss': 0.1774, 'learning_rate': 8.082857142857144e-06, 'epoch': 0.58}\n","{'loss': 0.1626, 'learning_rate': 8.011428571428573e-06, 'epoch': 0.59}\n","{'loss': 0.1716, 'learning_rate': 7.94e-06, 'epoch': 0.6}\n","{'loss': 0.1736, 'learning_rate': 7.86857142857143e-06, 'epoch': 0.62}\n","{'loss': 0.1684, 'learning_rate': 7.797142857142858e-06, 'epoch': 0.63}\n","{'loss': 0.1522, 'learning_rate': 7.725714285714286e-06, 'epoch': 0.64}\n","{'loss': 0.1673, 'learning_rate': 7.654285714285715e-06, 'epoch': 0.65}\n","{'loss': 0.1667, 'learning_rate': 7.5828571428571444e-06, 'epoch': 0.66}\n","{'loss': 0.155, 'learning_rate': 7.511428571428572e-06, 'epoch': 0.68}\n","{'loss': 0.1714, 'learning_rate': 7.440000000000001e-06, 'epoch': 0.69}\n","{'loss': 0.167, 'learning_rate': 7.36857142857143e-06, 'epoch': 0.7}\n","{'loss': 0.1514, 'learning_rate': 7.297142857142858e-06, 'epoch': 0.71}\n","{'loss': 0.156, 'learning_rate': 7.225714285714286e-06, 'epoch': 0.73}\n","{'loss': 0.1519, 'learning_rate': 7.154285714285715e-06, 'epoch': 0.74}\n","{'loss': 0.1575, 'learning_rate': 7.082857142857143e-06, 'epoch': 0.75}\n","{'loss': 0.1755, 'learning_rate': 7.011428571428572e-06, 'epoch': 0.76}\n","{'loss': 0.1403, 'learning_rate': 6.9400000000000005e-06, 'epoch': 0.78}\n","{'loss': 0.1671, 'learning_rate': 6.868571428571429e-06, 'epoch': 0.79}\n","{'loss': 0.1503, 'learning_rate': 6.797142857142858e-06, 'epoch': 0.8}\n","{'loss': 0.1674, 'learning_rate': 6.725714285714287e-06, 'epoch': 0.81}\n","{'loss': 0.1742, 'learning_rate': 6.654285714285716e-06, 'epoch': 0.82}\n","{'loss': 0.1655, 'learning_rate': 6.582857142857143e-06, 'epoch': 0.84}\n","{'loss': 0.1385, 'learning_rate': 6.511428571428572e-06, 'epoch': 0.85}\n","{'loss': 0.1439, 'learning_rate': 6.440000000000001e-06, 'epoch': 0.86}\n","{'loss': 0.1338, 'learning_rate': 6.368571428571429e-06, 'epoch': 0.87}\n","{'loss': 0.1524, 'learning_rate': 6.297142857142857e-06, 'epoch': 0.89}\n","{'loss': 0.16, 'learning_rate': 6.225714285714286e-06, 'epoch': 0.9}\n","{'loss': 0.164, 'learning_rate': 6.1542857142857145e-06, 'epoch': 0.91}\n","{'loss': 0.1486, 'learning_rate': 6.0828571428571435e-06, 'epoch': 0.92}\n","{'loss': 0.1543, 'learning_rate': 6.011428571428572e-06, 'epoch': 0.94}\n","{'loss': 0.152, 'learning_rate': 5.94e-06, 'epoch': 0.95}\n","{'loss': 0.1434, 'learning_rate': 5.868571428571429e-06, 'epoch': 0.96}\n","{'loss': 0.1546, 'learning_rate': 5.797142857142858e-06, 'epoch': 0.97}\n","{'loss': 0.1567, 'learning_rate': 5.725714285714287e-06, 'epoch': 0.98}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72bbb2b7b915485592a23d154cba2d2f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1204 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.19007627665996552, 'eval_wer': 34.499853472697076, 'eval_runtime': 5903.9872, 'eval_samples_per_second': 1.631, 'eval_steps_per_second': 0.204, 'epoch': 0.98}\n"]},{"ename":"PermissionError","evalue":"[WinError 32] Процесс не может получить доступ к файлу, так как этот файл занят другим процессом: './whisper-small-ru\\\\tmp-checkpoint-2000' -> './whisper-small-ru\\\\checkpoint-2000'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mf:\\prog\\anaconda\\envs\\fpmi_py\\lib\\site-packages\\transformers\\trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1541\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mf:\\prog\\anaconda\\envs\\fpmi_py\\lib\\site-packages\\transformers\\trainer.py:1920\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1920\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[1;32mf:\\prog\\anaconda\\envs\\fpmi_py\\lib\\site-packages\\transformers\\trainer.py:2275\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2275\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[1;32mf:\\prog\\anaconda\\envs\\fpmi_py\\lib\\site-packages\\transformers\\trainer.py:2391\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2387\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmain_process_first(\n\u001b[0;32m   2388\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRenaming model checkpoint folder to true location\u001b[39m\u001b[38;5;124m\"\u001b[39m, local\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_on_each_node\n\u001b[0;32m   2389\u001b[0m     ):\n\u001b[0;32m   2390\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(staging_output_dir):\n\u001b[1;32m-> 2391\u001b[0m             \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2393\u001b[0m \u001b[38;5;66;03m# Maybe delete some older checkpoints.\u001b[39;00m\n\u001b[0;32m   2394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n","\u001b[1;31mPermissionError\u001b[0m: [WinError 32] Процесс не может получить доступ к файлу, так как этот файл занят другим процессом: './whisper-small-ru\\\\tmp-checkpoint-2000' -> './whisper-small-ru\\\\checkpoint-2000'"]}],"source":["# Запускаем обучение\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"xeuTxo2M-Ts9"},"source":["# Выводим результаты обучения"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sa_pc9rwwx2i"},"outputs":[],"source":["# Функция сбора WER\n","import os\n","from pathlib import Path\n","import json\n","check_list = []\n","wer_list = []\n","df = pd.DataFrame()\n","for i in Path(output_dir).rglob('*.json'):\n","    if 'trainer_state' in str(i):\n","        f = open(os.path.abspath(i))\n","        data = json.load(f)\n","        check_list.append(data['global_step'])\n","        wer_list.append(data['best_metric'])\n","df['Checkpoint'] = check_list\n","df['WER'] = wer_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjZEG415wx2i","outputId":"de141f0a-db4e-4764-f647-7906ff1ba922"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Checkpoint</th>\n","      <th>WER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000</td>\n","      <td>43.834864</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2000</td>\n","      <td>34.499853</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3000</td>\n","      <td>31.703624</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4000</td>\n","      <td>31.703624</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Checkpoint        WER\n","0        1000  43.834864\n","1        2000  34.499853\n","2        3000  31.703624\n","3        4000  31.703624"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Выводим результат обучения\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AX1e2IVrwx2k"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
